[["重點回顧.html", "4 11/22 重點回顧 4.1 刪除非英文評論 4.2 文本重清理 4.3 變數修正（時間&amp;按讚數） 4.4 轉換寬格式的必要？ 4.5 ML model selection", " 4 11/22 重點回顧 4.1 刪除非英文評論 # 讀取資料 library(dplyr) data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_with_likes_1114.csv&quot;) # 使用正則表達式檢測僅包含英語字母、數字和標點的評論 data_clean &lt;- data %&gt;% mutate(comment = as.character(comment)) %&gt;% filter(grepl(&quot;^[A-Za-z0-9\\\\s.,!?\\\\&#39;\\&quot;-]+$&quot;, comment)) # 保存清理後的結果 write.csv(data_clean, &quot;cleaned_youtube_comments.csv&quot;, row.names = FALSE) cat(&quot;清理完成，剩餘&quot;, nrow(data_clean), &quot;條英語系評論。\\n&quot;) install.packages(&quot;textcat&quot;) # 讀取資料並檢測語言 library(textcat) data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_with_likes_1114.csv&quot;) # 檢測評論語言 data_clean &lt;- data %&gt;% mutate(comment = as.character(comment), language = textcat(comment)) %&gt;% filter(language == &quot;english&quot;) # 過濾僅包含英語的評論 # 保存清理後的結果 write.csv(data_clean, &quot;cleaned_youtube_comments_t2.csv&quot;, row.names = FALSE) cat(&quot;清理完成，剩餘&quot;, nrow(data_clean), &quot;條英語系評論。\\n&quot;) 4.2 文本重清理 ## 加載套件 library(tm) library(SnowballC) library(jiebaR) # 用於中文分詞 library(tidytext) library(dplyr) library(ggplot2) library(textstem) # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/cleaned_youtube_comments_t2.csv&quot; comments_df &lt;- read.csv(file_path) # 假設你的 CSV 包含 &#39;comment&#39; 和 &#39;views&#39; 列 # 增加 line 編號作為每條評論的唯一標識 comments_df &lt;- comments_df %&gt;% mutate(line = row_number()) # 清理文本 comments_df$text &lt;- tolower(comments_df$comment) # 轉換為小寫 comments_df$text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot; &quot;, comments_df$text) # 移除標點符號 comments_df$text &lt;- gsub(&quot;[[:digit:]]&quot;, &quot; &quot;, comments_df$text) # 移除數字 comments_df$text &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, comments_df$text) # 移除多餘空白 # 將評論拆分為單詞並保留 line 和 views 信息 wordfile &lt;- comments_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) %&gt;% select(line, word, views, timestamp, likes) # 加載標準停用詞列表 stop_words &lt;- tidytext::stop_words # 移除停用詞 wordfile &lt;- wordfile %&gt;% anti_join(stop_words, by = &quot;word&quot;) # 擴展停用詞列表，加入縮寫詞和口語詞 custom_stop_words &lt;- data.frame(word = c(&quot;hai&quot;,&quot;bhai&quot;,&quot;don&quot;,&quot;music&quot;,&quot;listen&quot;, &quot;song&quot;,&quot;ina&quot;,&quot;video&quot;,&quot;watch&quot;,&quot;day&quot;, &quot;edit&quot;, &quot;ve&quot;, &quot;ll&quot;, &quot;na&quot;, &quot;el&quot;, &quot;la&quot;, &quot;️️️&quot;, &quot;’ll&quot;, &quot;‘till &quot;, &quot;’ve&quot;, &quot;‘littl&quot;, &quot;’re&quot;, &quot;“’m&quot;, &quot;“’re&quot;, &quot;“bon&quot;, &quot;“copi&quot;, &quot;“don’t&quot;,&quot;you&quot;, &quot;the&quot;, &quot;and&quot;, &quot;you&quot;, &quot;that&quot;, &quot;for&quot;, &quot;doesn&quot;,&quot;didn&quot;,&quot;facemoji&quot;,&quot;bio&quot;, &quot;interesrting&quot;,&quot;awesomeeee&quot;,&quot;asome&quot;, &quot;hardd&quot;,&quot;kocham&quot;,&quot;১ম&quot;,&quot;২০&quot;,&quot;৪০&quot;,&quot;á&quot;, &quot;aa&quot;,&quot;aaaaaaaaaaahhhhhhhhhhhh&quot;, &quot;aaaaaaaaaaa&quot;,&quot;aaaaaaaaaa&quot;,&quot;aaaaaaaa&quot;, &quot;aaa&quot;,&quot;aaaaaah&quot;,&quot;aaaaaand&quot;,&quot;aæaæaæaæaæa&quot;, &quot;aaj&quot;,&quot;aaliyah&quot;,&quot;aam&quot;,&quot;aani&quot;,&quot;ab&quot;)) # 移除自定義的停用詞 wordfile &lt;- wordfile %&gt;% anti_join(custom_stop_words, by = &quot;word&quot;) # 使用 lemmatize_words 進行詞形還原 wordfile$word &lt;- lemmatize_words(wordfile$word) # 移除數字 wordfile &lt;- wordfile %&gt;% filter(!grepl(&quot;\\\\d+&quot;, word)) # 重新計算詞頻 wordfreq &lt;- count(wordfile, word, sort = TRUE) # 查看前 20 個最常見的詞 wordfreqdf20 &lt;- wordfreq[1:20,] # 打印結果 wordfreqdf20 # 計算 TF-IDF wordfile_with_tfidf &lt;- wordfile %&gt;% count(line, word, views, timestamp, likes) %&gt;% # 計算每行每個單詞的頻率 bind_tf_idf(word, line, n) # 計算 TF-IDF # 查看結果 head(wordfile_with_tfidf) 4.3 變數修正（時間&amp;按讚數） # 創建清理 timestamp 的函數 clean_timestamp &lt;- function(timestamp) { # 移除 &quot;(已編輯)&quot; 的內容 timestamp &lt;- gsub(&quot;\\\\(已編輯\\\\)&quot;, &quot;&quot;, timestamp) # 處理不同的時間單位，並轉換為天數 timestamp_days &lt;- ifelse( grepl(&quot;年前&quot;, timestamp), as.numeric(gsub(&quot;年前&quot;, &quot;&quot;, timestamp)) * 365, # 年轉換為天 ifelse( grepl(&quot;個月前&quot;, timestamp), as.numeric(gsub(&quot;個月前&quot;, &quot;&quot;, timestamp)) * 30, # 月轉換為天 ifelse( grepl(&quot;週前&quot;, timestamp), as.numeric(gsub(&quot;週前&quot;, &quot;&quot;, timestamp)) * 7, # 週轉換為天 ifelse( grepl(&quot;天前&quot;, timestamp), as.numeric(gsub(&quot;天前&quot;, &quot;&quot;, timestamp)), # 天保持不變 0 # 無法識別的時間，預設為 0 ) ) ) ) return(timestamp_days) } library(dplyr) # 將 timestamp 轉換為統一的天數 wordfile_with_tfidf &lt;- wordfile_with_tfidf %&gt;% mutate( timestamp_days = clean_timestamp(timestamp) # 新增統一格式的天數欄位 ) library(dplyr) # 檢查哪些值轉換後是 NA problematic_likes &lt;- wordfile_with_tfidf %&gt;% filter(is.na(as.numeric(gsub(&quot;萬&quot;, &quot;&quot;, likes)))) %&gt;% select(likes) %&gt;% distinct() wordfile_with_tfidf &lt;- wordfile_with_tfidf %&gt;% mutate(likes = case_when( grepl(&quot;^\\\\d+(\\\\.\\\\d+)?萬$&quot;, likes) ~ as.numeric(sub(&quot;萬&quot;, &quot;&quot;, likes)) * 10000, # 處理「X萬」格式 grepl(&quot;^\\\\d+$&quot;, likes) ~ as.numeric(likes), # 處理純數字 TRUE ~ NA_real_ # 其他情況設為 NA )) # 檢查轉換後的結果 head(wordfile_with_tfidf$likes) # 假設 wordfile_with_tfidf 已經存在於 R 環境中 output_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/wordfile_with_tfidf.csv&quot; # 將資料框寫入 CSV 檔案 write.csv(wordfile_with_tfidf, file = output_path, row.names = FALSE) # 確認輸出 cat(&quot;檔案已輸出至：&quot;, output_path) 4.4 轉換寬格式的必要？ # 載入套件 library(dplyr) library(tidyr) # 讀取資料 data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/wordfile_with_tfidf.csv&quot;, stringsAsFactors = FALSE) data &lt;- data %&gt;% rename(line_id = line) # 資料預處理 data$line_id &lt;- as.factor(data$line_id) data$word &lt;- as.character(data$word) # 將資料轉換為寬格式（詞彙矩陣） tf_idf_matrix &lt;- data %&gt;% select(line_id, word, tf_idf) %&gt;% pivot_wider( names_from = word, values_from = tf_idf, values_fill = list(tf_idf = 0) ) # 提取每則留言的其他特徵 other_features &lt;- data %&gt;% group_by(line_id) %&gt;% summarize( views = first(views), likes = first(likes), timestamp = first(timestamp), timestamp_days = first(timestamp_days) ) # 合併詞彙矩陣與其他特徵 final_data &lt;- tf_idf_matrix %&gt;% left_join(other_features, by = &quot;line_id&quot;) # 處理缺失值 final_data$likes[is.na(final_data$likes)] &lt;- 0 # 设置观看次数的上限 upper_limit &lt;- 10000000 # 1000 万 # 对观看次数进行截断 final_data &lt;- final_data %&gt;% mutate(views_capped = ifelse(views &gt; upper_limit, upper_limit, views)) # 过滤数据 final_data_filtered &lt;- final_data %&gt;% filter(views &lt;= upper_limit) # 檢視最終的資料集 head(final_data) 4.5 ML model selection ####################################################### ## 長格式資料、不tune ####################################################### # 載入必要的套件 library(tidyverse) library(tidymodels) library(textrecipes) # 假設您的資料已讀取，並命名為 data # 資料包含以下欄位：line_id, word, views, timestamp, likes, n, tf, idf, tf_idf, timestamp_days # 定義觀看次數的區間和對應的分類 recode_views &lt;- function(views) { if (views &gt;= 500000 &amp; views &lt; 1000000) { return(1) } else if (views &gt;= 1000000 &amp; views &lt; 2000000) { return(2) } else if (views &gt;= 2000000 &amp; views &lt; 4000000) { return(3) } else if (views &gt;= 4000000 &amp; views &lt; 5000000) { return(4) } else if (views &gt;= 5000000 &amp; views &lt; 6000000) { return(5) } else if (views &gt;= 7000000 &amp; views &lt; 8000000) { return(6) } else if (views &gt;= 8000000 &amp; views &lt; 9000000) { return(7) } else if (views &gt;= 9000000 &amp; views &lt; 10000000) { return(8) } else if (views &gt;= 10000000 &amp; views &lt; 15000000) { return(9) } else if (views &gt;= 15000000) { return(10) } else { return(NA) } } # 應用編碼函數並移除缺失值 data &lt;- data %&gt;% mutate(views_category = sapply(views, recode_views)) %&gt;% drop_na(views_category) %&gt;% mutate(views_category = factor(views_category)) # 將目標變數轉換為因子 # 處理數值型特徵，將 likes 和 timestamp_days 轉換為數值型，並處理缺失值 data &lt;- data %&gt;% mutate( likes = as.numeric(likes), likes = ifelse(is.na(likes), 0, likes), timestamp_days = as.numeric(timestamp_days), timestamp_days = ifelse(is.na(timestamp_days), 0, timestamp_days) ) # 分割資料（確保每個 line_id 只出現在訓練或測試集之一） set.seed(1234) line_ids &lt;- unique(data$line_id) train_ids &lt;- sample(line_ids, size = 0.7 * length(line_ids)) train_data &lt;- data %&gt;% filter(line_id %in% train_ids) test_data &lt;- data %&gt;% filter(!line_id %in% train_ids) train_data &lt;- train_data %&gt;% dplyr::select(-c(tf, idf, timestamp, views)) ml_recipe &lt;- recipe(views_category ~ word , data = train_data) %&gt;% step_dummy(all_nominal_predictors(), -all_outcomes()) %&gt;% step_normalize(all_numeric_predictors()) ml_recipe &lt;- recipe(views_category ~ word + tf_idf , data = train_data) %&gt;% # 移除不必要的欄位（若有） step_other(word, threshold = 0.01) %&gt;% # 將出現頻率低於1%的詞彙歸類為 &quot;other&quot; step_dummy(word) %&gt;% # 將 &#39;word&#39; 轉換為虛擬變數 step_normalize(all_numeric_predictors()) # 標準化數值型預測變數 # 檢查配方是否正確 prep(ml_recipe, training = train_data) %&gt;% juice() %&gt;% head() # 建立模型規格（未調參） lasso_spec &lt;- multinom_reg(penalty = 0.1, mixture = 1) %&gt;% # LASSO 邏輯回歸 set_engine(&quot;glmnet&quot;) svm_spec &lt;- svm_linear(cost = 1) %&gt;% # 線性 SVM set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;LiblineaR&quot;) null_spec &lt;- null_model() %&gt;% # Null 模型 set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;parsnip&quot;) # 建立工作流程 lasso_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(lasso_spec) svm_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(svm_spec) null_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(null_spec) # 訓練模型 final_lasso_fit &lt;- fit(lasso_workflow, data = train_data) final_svm_fit &lt;- fit(svm_workflow, data = train_data) final_null_fit &lt;- fit(null_workflow, data = train_data) # 預測與評估 # 定義評估指標 eval_metrics &lt;- metric_set(accuracy, kap, roc_auc) # LASSO 模型評估 lasso_predictions &lt;- predict(final_lasso_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(predict(final_lasso_fit, test_data)) %&gt;% bind_cols(test_data %&gt;% select(views_category)) lasso_metrics &lt;- lasso_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;LASSO 模型評估結果：&quot;) print(lasso_metrics) # SVM 模型評估 svm_predictions &lt;- predict(final_svm_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(predict(final_svm_fit, test_data)) %&gt;% bind_cols(test_data %&gt;% select(views_category)) svm_metrics &lt;- svm_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;SVM 模型評估結果：&quot;) print(svm_metrics) # Null 模型評估 null_predictions &lt;- predict(final_null_fit, test_data) %&gt;% bind_cols(test_data %&gt;% select(views_category)) null_metrics &lt;- null_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;Null 模型評估結果：&quot;) print(null_metrics) # 合併所有模型的評估結果 all_metrics &lt;- bind_rows( lasso_metrics %&gt;% mutate(model = &quot;LASSO&quot;), svm_metrics %&gt;% mutate(model = &quot;SVM&quot;), null_metrics %&gt;% mutate(model = &quot;Null Model&quot;) ) print(&quot;所有模型的評估結果：&quot;) print(all_metrics) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
